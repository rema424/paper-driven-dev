# CS1 条件D: Structured Checklist — RAG引用リナンバリング (Run 2)

> **Run**: CS1-D-Run2
> **Date**: 2026-02-22
> **Model**: Claude Sonnet 4.6
> **Context**: Fresh agent invocation

---

## 1. 問題の背景と文脈

RAG（Retrieval-Augmented Generation）システムでは、LLMが外部ドキュメントを検索・参照しながら回答を生成する。検索結果は内部的に `source_3`、`source_7` のようなIDで管理されるが、ユーザーには `[1]`、`[2]` のような連番での表示が求められる。

ストリーミング生成（Server-Sent Events や WebSocket によるチャンク単位の逐次出力）を採用することで、応答の先頭から即座にテキストがユーザーに届く。これはUX上の大きな利点である。しかし、ストリーミングの性質上、LLMが最終的にどの引用を使用するかは生成が完了するまで確定しない。

例えば、LLMが回答の先頭近くで `source_3` を参照し `[1]` として表示した後、回答の途中で `source_7` を参照し `[2]` として表示したとする。しかしストリーム完了後に「実際に使われた引用」を集計し直すと、順序が変わったり、番号の歯抜けが生じたりする可能性がある。これがユーザーに混乱を与え、信頼性を損なう。

この問題は、大規模なRAGシステムや、複数のソースを横断して引用する学術・ビジネス用途において特に顕在化する。

---

## 2. 対立する要求やトレードオフ

### トレードオフ A: リアルタイム性 vs. 正確性

**リアルタイム性の要求**: ユーザーは回答が生成されるにつれてリアルタイムに内容を読み進めたい。引用番号もその場で表示されることで、ユーザーはどの情報源に基づいているかを把握しながら読める。

**正確性の要求**: 一度表示した引用番号は変化してはならない。番号の変動はUXを壊し、ユーザーの信頼を損なう。しかしストリーミング途中では全体像が不明なため、「正しい」連番を確定できない。

これら二つは本質的に矛盾する。完全な正確性を求めるならば、ストリーミングが完了するまで引用表示を遅延させるしかなく、リアルタイム性を犠牲にする。

### トレードオフ B: 番号の連続性 vs. 実装のシンプルさ

**連続性の要求**: ユーザーには `[1]`、`[2]`、`[3]` と連続した番号で表示したい。番号に歯抜け（`[1]`、`[3]` のみ）があると視覚的に不自然であり、ソース一覧との整合が崩れる。

**実装のシンプルさの要求**: 引用の出現順に番号を付けるだけなら実装は単純だが、ストリーム完了後の後処理（リナンバリング）が必要になると、クライアント・サーバ間のプロトコルが複雑化し、エラーの発生源が増える。

特に、「ストリーム途中で表示した番号を後から変えない」という制約と「連番でなければならない」という制約が同時に課されると、両立の難度が大幅に上昇する。

### トレードオフ C: ソース事前フィルタリングの精度 vs. 応答品質

**精度の要求**: 引用問題を回避するために、LLMが生成前に「使用する引用の集合」を先に確定させる（プリフェッチ方式）アプローチがある。しかし、LLMは生成しながら必要な引用を動的に判断することが多く、事前確定は応答品質の低下につながりうる。

**応答品質の要求**: LLMが柔軟に引用を選択できる状態を保ちたいが、それではストリーミング開始時に引用番号が確定しない。

---

## 3. 分析の対象範囲

本分析が対象とするのは以下のシステム構成・レイヤーである。

- **LLM生成レイヤー**: 検索結果（内部ソースID）を受け取り、テキストをストリーミング生成するLLM
- **引用マッピングレイヤー**: 内部ソースIDをユーザー向け連番に変換するロジック
- **ストリーミング配信レイヤー**: クライアントへのチャンク配信プロトコル（SSE、WebSocketなど）
- **クライアント表示レイヤー**: ストリームを受信し、リアルタイムにテキストと引用を描画するUI

対象外とするもの:
- 検索（Retrieval）フェーズの最適化
- LLMのファインチューニングや引用精度向上
- 永続的なドキュメント管理システムの設計

---

## 4. 現状のアーキテクチャと制約

### 典型的なRAGストリーミングアーキテクチャ

```
[ユーザー]
    ↓ クエリ
[RAGオーケストレーター]
    ↓ 検索クエリ
[ベクトルDB / 検索エンジン]
    ↓ 検索結果 (source_1, source_3, source_7 など)
[LLM] ← プロンプトにソースIDと本文を埋め込む
    ↓ ストリーミング生成
[ストリーミングプロキシ]
    ↓ SSE / WebSocket
[クライアントUI]
```

### 制約

- **LLMは引用を生成中に動的に決定する**: プロンプトで指定されたソースのうち、どれを参照するかはLLMの生成過程に依存し、事前に確定できない
- **チャンクは不完全なテキスト単位で届く**: `[source` と `_3]` が別チャンクで届く可能性がある
- **クライアントは順次描画する**: 一度描画した文字を後から修正するにはDOM操作が必要で、アーキテクチャが複雑化する
- **ネットワーク遅延**: サーバ側でバッファリングしすぎると、ストリーミングの恩恵が失われる

---

## 5. 既存の解決アプローチとその限界

### 5.1 アプローチ1: ポストプロセス方式（完了後リナンバリング）

**手法**: ストリーム完了後に、生成されたテキスト全体を走査して引用IDを抽出し、出現順に連番を付け直してからクライアントに送信する。

**利点**:
- 実装がシンプル
- 引用番号の整合性が完全に保証される
- ストリーム完了後の確定した状態を扱うため、エラーが発生しにくい

**限界**:
- ストリーミングの恩恵がほぼ失われる。全文が完成するまでユーザーは何も見られない
- 長い回答では待機時間が顕著になる
- UXの観点から、ユーザーの体感応答速度が大幅に低下する

### 5.2 アプローチ2: 事前割り当て方式（プレフィルタリング）

**手法**: LLMに生成を開始させる前に「使用するソースのリスト」を決定させる別のLLM呼び出しを行い、そこで確定した順序で番号を割り当ててからストリーミング生成を開始する。

**利点**:
- ストリーミング中に引用番号が変動しない
- 事前にソース一覧をUI上に表示することができる

**限界**:
- 2回のLLM呼び出しが必要で、レイテンシが増加する
- 事前フィルタリング結果と実際の生成で使用する引用がずれる可能性がある（整合性問題）
- LLMは生成中に引用を追加する傾向があり、事前確定が困難

### 5.3 アプローチ3: 出現順マッピング方式（ファーストカム方式）

**手法**: ストリーム中に新たな内部ソースIDが初めて出現した時点で、そのIDに次の連番を割り当てる。マッピングテーブル（`source_3` → `[1]`、`source_7` → `[2]`）をサーバ側で管理し、ストリームと並走して更新する。

**利点**:
- 一度割り当てた番号は変化しない（再割り当て不要）
- ストリーミングを止めずに引用番号を確定できる
- 実装が比較的シンプル

**限界**:
- 引用が回答の後半に集中する場合、前半のテキストを表示しながら「まだ番号が確定していない引用プレースホルダー」を扱う必要が生じる
- チャンクが引用IDの途中で分割される場合の処理が必要（バッファリングロジック）
- ソース一覧の表示タイミングが問題になる（いつ確定するか不明）

### 5.4 アプローチ4: プレースホルダー + 後置き差分パッチ方式

**手法**: ストリーム中は `[REF:source_3]` のような識別可能なプレースホルダーを表示しておき、ストリーム完了後に差分パッチ（番号確定マッピング）をクライアントに送信し、クライアントがDOMを更新する。

**利点**:
- ストリーミング中もテキスト本体はリアルタイム表示可能
- 引用番号の整合性はストリーム完了後に保証できる

**限界**:
- 「一度表示した番号が変わらない」という要件を満たさない（プレースホルダーが番号に変化するため、見た目上の変動がある）
- クライアント側のDOM更新ロジックが複雑になる
- プレースホルダーが表示されている間のUXが悪い

---

## 6. 問題の本質的な困難

この問題の本質的な困難は、**「情報の確定タイミングの非同期性」と「表示の不変性要求」の根本的な矛盾**にある。

具体的には以下の3点が問題の核心を構成する。

**困難1: 部分観測問題**
ストリームの途中では、全体のうちのどの引用が最終的に使用されるかを観測できない。番号付けを「正しく」行うためには全体を知る必要があるが、全体を知るためにはストリームが完了するまで待つ必要がある。これは部分観測マルコフ決定過程（POMDP）に類似した構造をもつ。

**困難2: コミットメント問題**
一度ユーザーに表示した情報は「コミットされた」状態になる。UXの観点では、表示済みの数字を書き換えることはユーザーの認知負荷を高め、信頼を損なう。しかし正確な番号を表示するためにはコミットメントを遅延させる必要がある。

**困難3: チャンク粒度の不確実性**
LLMのストリーミング出力は文字単位や単語単位の任意の粒度で届く。引用ID（例: `[source_3]`）が複数チャンクに分割される可能性があり、リアルタイムなパターンマッチングには先読みバッファが必要になる。

---

## 7. 解決策

### 7.1 基本原理

**「出現順確定・単調増加マッピング」原理**を採用する。

核心的な設計決定は以下の通りである:

- **引用番号の割り当てはファーストカム・ファーストサーブで行う**: ある内部ソースIDがストリーム中に初めて出現した瞬間に、その時点で次に割り当て可能な連番を永続的に付与する
- **マッピングは単調増加のみ**: 一度付与した番号は変更しない。これにより「一度表示した番号が変わらない」が保証される
- **ソース一覧は動的に追加**：ストリーム中に新たな引用が出現するたびにソース一覧の末尾に追記する形式とする。ストリーム完了時点でソース一覧と本文の整合性が自動的に保たれる

この原理は、引用番号の「連続性」を「出現した順に連続」と再定義することで成立する。最終的な番号順序は出現順であり、任意の意味論的順序（重要度順など）ではない。ただしこれは通常の学術文書でも採用されている慣習と一致する。

### 7.2 実装方針

#### サーバ側: ストリーミングパイプライン

```
[LLM ストリームチャンク]
         ↓
[引用IDバッファリングパーサー]
  - チャンクを受け取り、引用IDパターン ([source_N]) を検出
  - パターンが複数チャンクにまたがる場合は先読みバッファに保持
  - 完全な引用IDを検出したらマッピングレイヤーに通知
         ↓
[引用マッピングテーブル (インメモリ)]
  - source_id → display_number の辞書
  - 新規ソースIDが来たら max(current_numbers) + 1 を付与
  - 既存ソースIDが来たら既存番号を返す
         ↓
[変換済みチャンク出力]
  - [source_3] → [1] のように置換してクライアントに送信
  - 新規引用が出現した場合は「新規ソースイベント」も併送
         ↓
[クライアント SSE / WebSocket]
```

#### クライアント側: プログレッシブ表示

```
[チャンク受信]
  - テキストチャンク: そのままDOM末尾に追記
  - 新規ソースイベント: ソース一覧に新エントリを追加
  - ストリーム完了イベント: ソース一覧を確定状態に更新（UI変化なし）
```

#### 引用IDパーサーの設計

引用IDの検出には有限状態機械（FSM）を使用する。

```
状態:
  NORMAL      → 通常テキスト出力中
  BRACKET_OPEN  → '[' を検出、先読みバッファに保持
  IN_SOURCE   → 'source_' パターンの途中
  COMPLETE    → ']' で完了、マッピング実行
  FALLTHROUGH → パターン不一致、バッファを通常テキストとして出力

遷移:
  NORMAL + '[' → BRACKET_OPEN
  BRACKET_OPEN + 's' → IN_SOURCE
  BRACKET_OPEN + その他 → FALLTHROUGH → NORMAL
  IN_SOURCE + ']' → COMPLETE
  IN_SOURCE + タイムアウト → FALLTHROUGH → NORMAL
```

#### プロトコル設計（SSEイベントタイプ）

```
event: text
data: {"content": "この研究によれば、"}

event: citation
data: {"display_number": 1, "source_id": "source_3", "title": "Smith et al. 2024", "url": "..."}

event: text
data: {"content": " [1] "}

event: text
data: {"content": "次の調査でも同様の結果が示されており、"}

event: citation
data: {"display_number": 2, "source_id": "source_7", "title": "Lee et al. 2023", "url": "..."}

event: text
data: {"content": " [2] "}

event: done
data: {"total_citations": 2}
```

このプロトコルにより:
- テキストと引用メタデータが分離して届く
- クライアントはcitationイベントを受け取った時点でソース一覧を追記できる
- ストリーム完了時に番号とソース一覧の整合性が自動的に保たれる

---

## 8. 検証条件

### シナリオ1: 基本的な複数引用の出現順マッピング

**Given**:
- LLMが source_7 を参照するテキストを先に生成し、その後 source_3 を参照するテキストを生成する
- 検索結果には source_3、source_7 の両方が含まれている

**When**:
- ユーザーがクエリを送信し、ストリーミングが開始される

**Then**:
- source_7 が最初に出現するため `[1]` が割り当てられ、クライアントに `[1]` として表示される
- その後 source_3 が出現し `[2]` が割り当てられ、クライアントに `[2]` として表示される
- ストリーム完了後のソース一覧は `[1] source_7`、`[2] source_3` の順になる
- ストリーム完了前後で、クライアントに表示されている番号に変化はない

### シナリオ2: 同一ソースの複数回参照

**Given**:
- LLMが回答の前半と後半の両方で source_3 を参照する

**When**:
- ストリーミング中に source_3 が2回出現する

**Then**:
- 最初の出現時に `[1]` が割り当てられる
- 2回目の出現時も同じ `[1]` が使用される（新たな番号は割り当てられない）
- ソース一覧に source_3 は1つのエントリのみ存在する

### シナリオ3: 引用IDがチャンク境界で分割される

**Given**:
- ストリームのあるチャンクが `[source_` で終わり、次のチャンクが `3]` で始まる

**When**:
- 2つのチャンクがシーケンシャルに到着する

**Then**:
- FSMは `[source_` の受信時点で BRACKET_OPEN → IN_SOURCE 状態に遷移し、テキスト出力を保留する
- 次のチャンク `3]` の受信後、`[source_3]` として完全に認識される
- マッピングが実行され、`[1]`（または既存の番号）として出力される
- ユーザーには分割されたことが分からず、自然に `[1]` が表示される

### シナリオ4: ストリーム完了後の整合性確認

**Given**:
- ストリーム中に source_3、source_7、source_1 が出現順にこの順で出現した

**When**:
- ストリームが完了し、done イベントが送信される

**Then**:
- ソース一覧は `[1] source_3`、`[2] source_7`、`[3] source_1` の順になる
- 本文中の引用番号は `[1]`、`[2]`、`[3]` であり、ソース一覧と完全に整合する
- 番号に歯抜けは存在しない

---

## 9. 制約、限界、今後の課題

### 制約

**意味論的順序の欠如**: 本方式では引用番号は「出現順」であり、「重要度順」や「ドキュメント日付順」ではない。学術文書での慣例（Vancouver方式）とは一致するが、特定の用途では出現順が不自然に感じられる場合がある。

**LLMのプロンプト設計依存**: LLMが `[source_N]` 形式で確実に引用を埋め込むようにするには、プロンプトエンジニアリングと出力フォーマットの制御が必要である。これは本方式の実装において外部依存となる。

### 限界

**引用の遡及削除には対応しない**: LLMが誤って引用を生成し、後のテキストでそれを撤回した場合、一度表示した番号とソース一覧エントリを削除することは「番号不変」原則と矛盾するため対応しない。この場合、不要なソースエントリが末尾に残ることになる。

**長い引用IDパターンのタイムアウト**: FSMが BRACKET_OPEN または IN_SOURCE 状態で長時間とどまる場合（ネットワーク遅延）のタイムアウト設定が必要であり、そのしきい値設計はシステム依存である。

**マルチターン会話への拡張**: 会話の複数ターンにわたって引用番号を通算するか、ターンごとにリセットするかのポリシー設計が別途必要になる。

### 今後の課題

**LLM側でのStructured Output活用**: 将来的には、LLMのStructured Output機能（JSON mode等）を使用して、生成テキストと引用リストを構造化された形式で分離出力させることが可能になると考えられる。この場合、引用の後処理がより確実になる。

**ストリーミングJSON対応**: JSON Streamingプロトコル（ndjsonなど）を採用することで、テキストと引用メタデータをより厳密に分離した形で伝送できる。ただし、クライアント側のパーサー複雑度が増加する。

**引用の信頼スコア統合**: 検索エンジンが返す関連性スコアを引用メタデータに含め、UIで信頼度を視覚化することで、引用の品質をユーザーに伝えることができる。本方式のプロトコル設計（citationイベント）はこの拡張を自然にサポートできる。
