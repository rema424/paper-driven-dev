# LLM に「論文を書け」と言うだけで、設計分析の品質向上が観測される — Paper-Driven Development の提案

> **実験条件**: Model: Claude Opus 4 | N: 1 (RAG citation rendering) | Date: 2026-02 | Evaluator: 著者
> **注意**: 定量比較データ（comparison-data.md）は別条件（Model: o3, N: 2）で取得。同一実験ではない。

## はじめに

LLM に設計上の問題を相談するとき、あなたはどんなプロンプトを書いていますか？

「この問題の解決策を提案してください」——おそらく多くの人がこう聞くでしょう。そして LLM は、正しい答えを返してくれます。

でも私は偶然、もっと良い聞き方を見つけました。

**「この問題について学術論文を書いてください」**

たったこれだけで、LLM の出力が大きく変わります。テンプレートが既存手法の比較分析やトレードオフの形式的定義、テスト条件の導出を構造的に引き出してくれる。この現象を体系化したのが **Paper-Driven Development（論文駆動開発）** です。

## 発見のきっかけ

ある日、RAG システムの設計問題——ストリーミング中に引用番号をリアルタイムで正しく表示する方法——に取り組んでいました。

最初は普通に「解決策を提案して」と聞きました。LLM は正しいアルゴリズムを提案してくれました。でも何か物足りない。「本当にこれがベストなのか？」「他の方法は検討しなくていいのか？」という疑問が残りました。

そこで思いつきで「この問題について学術論文の形式で書いて」と頼んでみました。

結果は驚くべきものでした。

## Before/After: 同じ問題、違う聞き方

### Before: 通常のプロンプト

```
この問題の設計分析と解決策を提案してください。
```

LLM の出力（要約）:

- **結論**: サーバー側で逐次マッピングする設計が最適
- **実装**: TypeScript のコード例
- **注意点**: append-only ストリームが前提

約40行。正しい答えです。

### After: 論文形式のプロンプト

```
この問題について学術論文の形式で書いてください。
（§1 問題定義 〜 §7 制約と今後の課題）
```

LLM の出力（要約）:

- **§1**: リアルタイム表示と正しい連番という**矛盾する2つの要求**を形式的に定義
- **§3**: **5つの既存アプローチ**（スピナー表示、手動パーサー、LLM 直接採番、ギャップ許容、仮番号差し替え）を手法/利点/限界で分析
- **§4**: 問題の本質——採番順序の非決定性——を具体例で証明
- **§5**: 提案手法の不変条件を数学的に記述（Append-Only 性質）
- **§6**: **4つのテスト可能な性質**を定義

約270行。同じ結論に到達しますが、そこに至るプロセスの質がまったく違います。

> **注意**: Before プロンプトは最適化していない（CoT やペルソナプロンプティング等は未使用）。強化版プロンプトとの比較は将来課題である。

### 何が違うのか

| 観点 | Before | After |
| --- | --- | --- |
| 既存手法の分析 | 0件 | 5件 |
| トレードオフの定義 | 暗黙的 | 形式的（矛盾する2要求） |
| 提案の正当化 | 「最適です」 | 5手法の消去法 + 不変条件の証明 |
| テスト条件 | 0件 | 4件 |
| 制約の明示 | 1件 | 3件 |

通常プロンプトは「何をするか」を教えてくれます。論文形式は「なぜそうするか」「他ではなぜダメか」「どう検証するか」まで教えてくれます。

## なぜ「論文を書け」で品質が上がるのか

正直に言うと、因果関係は検証できていません。しかし3つの仮説があります。

### 仮説 1: 学習データ品質効果

学術論文は査読済みの高品質テキストです。LLM は膨大な論文データで訓練されており、「論文を書け」というプロンプトが、高品質な出力パターンを活性化している可能性があります。

### 仮説 2: 暗黙的 Chain-of-Thought

論文形式は「問題定義→既存手法分析→本質の特定→提案→検証」という思考順序を**構造的に強制**します。これが意図せず Chain-of-Thought プロンプティングとして機能しているのかもしれません。

### 仮説 3: ペルソナ効果

「論文を書け」は暗黙的に「あなたは研究者です」というペルソナを設定します。研究者ペルソナは、批判的思考、体系的分析、限界の明示といった行動パターンを誘発します。

どの仮説が正しいかはわかりません。しかし、ケーススタディでは一貫して、論文形式のテンプレートを使った場合に出力の品質向上が観測されています。

## テンプレート

体系化した結果、以下の7セクション構造が効果的だとわかりました。

```
§1. 問題定義
  §1.1 背景
  §1.2 矛盾する要求（対立するトレードオフ）
  §1.3 本文書の範囲
§2. 現状のアーキテクチャと制約
§3. 既存アプローチとその限界
§4. 問題の本質
§5. 提案手法
§6. 検証可能な性質
§7. 制約と今後の課題
```

特に重要なのは:

- **§1.2**: 矛盾する要求を2つ以上特定する。片方しかなければ、問題は自明であり、この手法は不要
- **§3**: 既存手法の「限界」を必ず書かせる。利点だけでは分析にならない
- **§6**: テスト条件を Given/When/Then 形式で定義させる。実装前にテスト設計が可能になる

## Claude Code プラグインとして公開しました

この手法を誰でも簡単に試せるよう、Claude Code のプラグインとして公開しています。

```shell
# インストール
/plugin marketplace add rema424/paper-driven-dev
/plugin install paper-driven-dev@rema424-paper-driven-dev

# 使う
/paper-driven-dev:article 認証システムのセッション管理方式の選定
```

GitHub リポジトリ: https://github.com/rema424/paper-driven-dev

## 今後の検証計画

この手法はまだ N=1 の観察に過ぎません。以下の検証を進めていきます。

1. **多ドメイン検証**: RAG 以外の問題（認証設計、パフォーマンス最適化、データモデリング等）で再現性を確認する
2. **定量比較**: 同じ問題を (a) 通常プロンプト (b) 仕様駆動形式 (c) 論文形式 で比較し、品質指標を定量化する
3. **多モデル検証**: Claude 以外の LLM（GPT、Gemini）でも同じ効果が得られるか確認する
4. **品質メトリクス**: 設計分析の品質を定量的に評価する指標を定義する

## 限界の正直な告白

最後に、この手法の限界を正直に書いておきます。

- **N=1**: 現時点ではサンプルが1つだけです。再現性は未検証
- **仮説未検証**: なぜ効くのかの3つの仮説は、いずれも因果関係が未確定
- **過剰適用リスク**: すべての問題に論文形式を適用する必要はありません。単純な機能追加やバグ修正には不要です
- **バイアスの可能性**: 論文形式の出力が「良く見える」だけで、実際の設計品質向上に繋がるかは別問題です
- **LLM 依存**: この手法の効果は LLM の訓練データと能力に依存しており、将来的に変わる可能性があります

それでも、プロンプトを変えるだけでこれだけの差が出るなら、試してみる価値はあると思います。

---

GitHub: https://github.com/rema424/paper-driven-dev

サンプル（Before/After 比較）: リポジトリの `docs/examples/` を参照
