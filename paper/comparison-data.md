# Quantitative Comparison Data: Conventional Prompting vs Paper-Driven Development

> **実験条件**: Model: OpenAI o3 (via Codex) | N: 2 (CS1: citation rendering, CS2: session management) | Date: 2026-02 | Evaluator: 著者
> **注意**: paper-driven-dev.md 本文中の定性分析は別条件（Model: Claude Opus 4, N: 1）で実施。同一実験ではない。

## Methodology

- **Model**: OpenAI o3 (via Codex), February 2026
- **Prompt language**: Japanese
- **Conventional prompt**: 「この問題の設計分析と解決策を提案してください」
- **PDD prompt**: §1–§7 template with section guidelines
- **Measurement**: Manual counting by authors (automated metrics are future work)

### Line Counting Rules

- **Total lines**: 出力全体の行数。空行を含む。
- **空行の扱い**: 空行はセクション区切りとして存在するため Total lines に含めるが、「内容のある行」として個別にカウントする指標（e.g., existing approaches analyzed）には含めない。
- **プロンプト行の除外**: ユーザーの入力プロンプト自体はカウント対象外。LLM の出力のみをカウントする。
- **コードブロック**: コードブロック内の行も Total lines に含める。
- **再現性**: 各出力の原文は `docs/examples/` に保存されており、カウントの再現が可能。

> **注意**: Before（Conventional）プロンプトは最適化していない。CoT やペルソナプロンプティング等を組み合わせた強化版プロンプトとの比較は将来課題である。

## Case Study 1: RAG Citation Renumbering

| Metric | Conventional | PDD | Delta |
| --- | ---: | ---: | ---: |
| **Total lines** | 40 | 269 | +229 (5.7x) |
| **Sections** | 4 (unstructured) | 7 (structured §1–§7) | +3 |
| **Existing approaches analyzed** | 0 | 5 | +5 |
| **Limitations per approach** | — | 1–2 each | — |
| **Conflicting requirements identified** | 0 (implicit) | 2 (formal) | +2 |
| **Formal invariants defined** | 0 | 1 (Append-Only) | +1 |
| **Testable properties** | 0 | 4 | +4 |
| **Constraints/limitations disclosed** | 1 | 3 | +2 |
| **Code examples** | 1 (algorithm) | 2 (algorithm + usage) | +1 |
| **Architecture diagrams** | 0 | 1 (ASCII) | +1 |
| **Concrete examples with data** | 0 | 1 (numbering table) | +1 |

### Correct conclusion reached?

Both: **Yes** — both identified first-occurrence body-order renumbering as the solution.

## Case Study 2: Multi-Tenant Session Management

| Metric | Conventional | PDD | Delta |
| --- | ---: | ---: | ---: |
| **Total lines** | 38 | 142 | +104 (3.7x) |
| **Sections** | 4 (unstructured) | 7 (structured §1–§7) | +3 |
| **Existing approaches analyzed** | 3 (comparison table) | 4 (detailed analysis) | +1 |
| **Limitations per approach** | 1 sentence each | 1–2 sentences each | deeper |
| **Conflicting requirements identified** | 0 (implicit) | 3 (formal) | +3 |
| **Formal invariants defined** | 0 | 1 (Versioned Session Authority) | +1 |
| **Testable properties** | 0 | 7 | +7 |
| **Constraints/limitations disclosed** | 0 | 3 + future work | +3 |
| **Code examples** | 1 (Redis keys) | 1 (Redis keys + JWT claims) | comparable |
| **Scope definition** | 0 | 1 (§1.3 explicit scope) | +1 |

### Correct conclusion reached?

Both: **Yes** — both recommended short-lived JWT + Redis session store + event-driven revocation.

## Aggregate Summary (N=2)

| Metric | Conventional (avg) | PDD (avg) | Ratio |
| --- | ---: | ---: | ---: |
| Total lines | 39 | 206 | 5.3x |
| Existing approaches analyzed | 1.5 | 4.5 | 3.0x |
| Conflicting requirements | 0 | 2.5 | — |
| Testable properties | 0 | 5.5 | — |
| Constraints disclosed | 0.5 | 3.0 | 6.0x |

## Qualitative Observations

### 1. Both approaches reach the same conclusion

In both case studies, the conventional prompt and PDD prompt arrived at essentially the same recommended solution. PDD does not help find *better* answers—it helps **justify and verify** answers.

### 2. PDD's primary advantage is verifiability

The most consistent difference is in testable properties: 0 vs 4 (Case 1), 0 vs 7 (Case 2). Conventional prompts produce implementations; PDD produces implementations + test specifications.

### 3. Trade-off identification is structurally absent in conventional prompting

Neither conventional output formally identified conflicting requirements. PDD's §1.2 template forces this analysis, producing 2 and 3 trade-offs respectively.

### 4. Constraint disclosure is consistently deeper in PDD

Conventional outputs disclosed 1 and 0 constraints respectively. PDD disclosed 3 in both cases. The §7 template forces honest acknowledgment of limitations.

### 5. Output length is 4–6x larger

PDD outputs are significantly longer. This is both a benefit (more thorough) and a cost (more tokens, more reading time). The additional length is concentrated in §3 (alternatives analysis) and §6 (testable properties)—sections with high practical value.

## Threats to Validity

1. **Small sample (N=2)**: Two case studies are insufficient for statistical conclusions
2. **Single model**: Both conventional and PDD outputs were generated by the same model (o3). Results may differ across models
3. **Author evaluation**: All metrics were counted by the authors, not independent evaluators
4. **Problem selection bias**: Both problems were non-trivial design challenges where PDD is expected to perform well. Performance on simpler problems is untested
5. **Prompt optimization**: The conventional prompt was not optimized (e.g., no CoT, no persona). A well-engineered conventional prompt might narrow the gap
6. **Language**: Both prompts were in Japanese. Results may vary in English or other languages
